##Masukkan perintah untuk memanggil dan mengolah data
import pandas as pd #proses analisis data
import numpy as np #membaca dan membuat operasional data vektor
import matplotlib.plyplot as plt #package visualisasi data plot atau grafik
import seaborn as sns #Package visualisasi data

##Preprocessing Data (Statistik deskriptif data)
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline #transformator data predict
from sklearn.metrics import r2_score, median_absolute_error 
from sklearn.metrics import mean_squared_error as mse

##Standarisasi Feature Variabel Rasio KEuangan 
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve, auc
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

##melatih model untuk regresi dan klasifikasi (estimasi)
from sklearn.ensemble import GradientBoostingClassifier 
from sklearn.model_selection import cross_val_score, learning_curve, GridSearchCV, train_test_split, learning_curve #Validasi model
from sklearn.feature_selection import RFECV #estimator, memilih jumlah fitur yang optimal

##Memanggil Data yang Akan Digunakan 
data = pd.read_csv('E:/Dataset ALiyya Rifanda 1401164086.csv')
data.head() #menampilkan 5 baris pertama data

##Jumlah Perusahaan yanga Diteliti (sebanyak 23 perusahaan)
print(data.Company.unique().shape)

##Melihat informasi data
data.head()
data.tail()
data.describe()

##Data Cleaning
plt.figure(figsize=(18,10))
sns.distplot(data['Distress'],kde=False, rug=True, color = 'blue');
plt.title('Financial Distress ')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

##Menampilkan deskriptif statistik data 
#Hanya kolom yang bertipe numerik yang akan ditampilkan statistiknya.
data['Financial Distress'].describe()

##Menghapus outliers, angka 100 didapat dari nilai maks
plt.figure(figsize=(17,8))
sns.distplot(data[data['Distress'] < 100]['Distress'], kde=False, rug=True, color = 'blue')
plt.title('Financial Distress Histogram without outlier')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

##Data Preparation, Normalisasi Data, Mereduksi nilai pada atribut.
#Metode yang digunakan adalah dengan memeriksa nilai tetangga kemudian menemukan nilai baru
pd.qcut(data['Distress'], 3).value_counts()

data['status'] = np.where(data['Distress'] < nT,0,np.where(data['Distress'] < nA, 1,2))

data['status'].value_counts()

##Seleksi Data untuk Modeling
#Menggunakan sampel tahun terkahir dari setiap perusahaan sebagai pemodelan. 
 Another holdout dataset for testing will be produced this is for the first period of the companies. We need to check if the last period is representative for every period.
 Jika tidak demikian, maka tidak akan mendapat informasi terkait data tsbt dan perlu memikirkan proses alternatif lainnya
 
data_one_period = data.drop_duplicates(subset=['Company'], keep='last')
 
data_one_period_test = data.drop_duplicates(subset=['Company'], keep='first')
data_final_test = data_one_period.drop(['Distress', 'Company'], axis=1)
data_final_test.head()

data_one_period.shape

##Dari hasil seleksi data dari dataset (data_one_period), didapat (row, column). Dimana distribusi financial distress terlah diubbah ke dalam kolom baru bernama 'status'
data_one_period['status'].value_counts()
data_one_period.head()

#kolom yang tidak berpengaruh akan dieleminasi, sehingga akan menghasilkan matriks korelasi
data_final = data_one_period.drop(['Distress', 'Company'], axis=1)
data_final.head()

##MEMILIH VARIAABEL DENGAN KOEFISIEN KORELASI
#Korelasi variabel prediktor terhadap kolomstatus bernilai absolut yang diurutkan dari nilai tertinggi.
#Menggunakan korelasi >10% , <70%
plt.figure(figsize=(20,10))
plt.title('Correlation Matrix')
sns.heatmap(data_final.corr(), annot=True)
plt.show()

abs(data_final.corr()['status']).sort_values(ascending=True)
variables = abs(data_final.corr()['status']).sort_values(ascending=True)

v1 = variables[0.05 < variables.values]
v2 = v1[v1.values < 0.7] #nilai baru sbg variabel prediktor
len(v2.index)

ind = v2.index
v2.index

y = data_final['status']
X = data_final[ind]

##MENGECEK HUBUNGAN ANTAR VARIABEL PREDIKTOR. VIF akan digunakan sebagai fungsi yang telah terluis untuk membantu dalam menyeleksi variabel.
def calc_vif(X):
    
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return(vif)
    
vif_df = calc_vif(X)
vif_df.shape   

vif_df_without_corr = vif_df[vif_df['VIF'] < 10] #nilai cut off 10
vif_df_without_corr.shape

vif_df_without_corr['variables']

X = X[vif_df_without_corr['variables']]
X.shape

plt.figure(figsize=(20,10))
plt.title('Correlation Matrix of the selected predictors')
sns.heatmap(X.corr(), annot=True)
plt.show()

##Menentukan hubungan antar variabel yang akan digunakan untuk prediksi financial distress dengan mengabaikan tahun dan perusahaan
correlation = data.drop(labels= ['Time', 'Company'], axis =1).corr()
correlation['Distress'].plot()

##Menampilkan Korelasi Feature (Variabel) yang memiliki pengaruh terhadap klasifikasi
f, ax = plt.subplots(figsize= (25,25))
sns.barplot(x = correlation['Distress'], y = correlation.index)
plt.xticks(fontsize = 15)
plt.yticks(fontsize = 15)
plt.ylabel('FEATURES', fontsize= 15)
plt.xlabel('VALUE OF FINANCIAL DISTRESS', fontsize= 15)
plt.title('FEATURE CORRELATION', fontsize =15)

##MODELING ON THE SELECTED DATA
X_train, X_test, y_train, y_test = train_test_split(
     X, y, test_size=0.3, random_state=42)
     
#Menulis fungsi evaluasi kinerja model menggunakan akurasi terbaik pada dataset latih dan dataset uji
#Kesalahan, dan laporan klasifikasi berdasarkan confussion matrix >>presisi, recall, f1-score, dan support
scaler =  MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

def evaluate(model, train_features, train_labels, test_features, test_labels):
    train_acc = model.score(train_features, train_labels)
    predictions = model.predict(test_features)
    rmse = np.sqrt(mean_squared_error(test_labels, predictions))
    accuracy = accuracy_score(test_labels, predictions)
    class_report = classification_report(test_labels, predictions)
    print('Model Performance')
    print('Training data performance {:0.2f}'.format(train_acc))
    print('Root Mean Squired Error: {:0.2f}'.format(rmse))
    print('Test Accuracy: {:0.2f}.'.format(accuracy))
    print(class_report)
    
##APPLYING SVM
#Support Vector Classifier, dari sekian kernel yang dimiliki tidak memberikan nilai akurasi yang tinggi, sehingga tidak digunakan.

sv_base_rbf = SVC(kernel='rbf').fit(X_train, y_train)
evaluate(sv_base_rbf,  X_train, y_train, X_test, y_test)

sv_base_sig = SVC(kernel='sigmoid').fit(X_train, y_train)
evaluate(sv_base_sig,  X_train, y_train, X_test, y_test)

#SVC kernel linear
sv_base_lin = SVC(kernel='linear').fit(X_train, y_train)
evaluate(sv_base_lin,  X_train, y_train, X_test, y_test)

##MEMBANGUN MODEL DARI RATA_RATA DATA
data_average = data.groupby("Company", as_index=False).mean() 
data_average = data_average.drop(['Time'], axis=1)
data_average.shape

data_average.head()

##We need to change the Time column with the full time. So we will drop the average age column and merge the tables with the one below to get the needed total Time periods.
time_to_merge = pd.DataFrame(data_one_period[['Company','Time']])
time_to_merge.head()

data_average_with_time = pd.merge(time_to_merge,data_average, on='Company')
data_average_with_time.head() #LINE58

plt.figure(figsize=(17,8))
sns.distplot(data_average['Financial Distress'],kde=False, rug=True, color = 'blue')
plt.title('Financial Distress of the data average Histogram')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

plt.figure(figsize=(17,8))
sns.distplot(data_average[data_average['Distress'] < 10]['Financial Distress'],kde=False, rug=True, color = 'blue')
plt.title('Financial Distress Histogram  of data average without outliers testing holdout')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

pd.qcut(data_average['Distress'], 3).value_counts()

data_average['status'] = np.where(data_average['Distress'] < nA baris akhir,0,np.where(data_average['Distress'] < nA atas, 1,2))

data_average = data_average.drop(['Distress'], axis=1)

abs(data_average.corr()['status']).sort_values(ascending=True)

variables = abs(data_average.corr()['status']).sort_values(ascending=True)

v12 = variables[0.05 < variables.values]
v22 = v12[v12.values < 0.7]
len(v22.index)

ind1 = v22.index
v22.index

y1 = data_final['status']
X1 = data_final[ind1]

vif_df1 = calc_vif(X1)
vif_df1.shape

##We change the VIF threshold to 7 to remove the multicolinearity.
vif_df_without_corr1 = vif_df1[vif_df1['VIF'] < 7]
vif_df_without_corr1.shape

X1 = X1[vif_df_without_corr1['variables']]
X1.shape

plt.figure(figsize=(20,10))
plt.title('Correlation Matrix of the selected predictors for data average')
sns.heatmap(X1.corr(), annot=True)
plt.show()

##Random Search with the best modelsÂ¶
rf_avg_tuned.best_params_

data.describe()

data[data['Company'] == 2]


data.loc[data['Company'] == 2].iloc[:,data.index[1]]

plt.figure(figsize=(17,8))
for i in range(len(data.columns[3:-1])):
    plt.plot(data.loc[data['Company'] == 2]['Time'], 
             data.loc[data['Company'] == 2].iloc[:,data.index[i]], label = 'i')
plt.title('Line plot of Xs during the obseved period.')
#plt.legend()
plt.xlabel('Time')
plt.ylabel('values')
plt.grid(True)
plt.show()

PART 2

from statsmodels.stats.outliers_influence import variance_inflation_factor
from pycaret.classification import *

data = pd.read_csv('E:/Dataset ALiyya Rifanda 1401164086.csv')

plt.figure(figsize=(18,10))
sns.distplot(data['Distress'],kde=False, rug=True, color = 'blue');
plt.title('Financial Distress Histogram')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

data['Financial Distress'].describe()

plt.figure(figsize=(17,8))
sns.distplot(data[data['Financial Distress'] < 100]['Financial Distress'], kde=False, rug=True, color = 'blue')
plt.title('Financial Distress Histogram without outlier')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

data_average = data.groupby("Company", as_index=False).mean() 
data_average = data_average.drop(['Time'], axis=1)
data_average.shape

data_average.head()

data_one_period = data.drop_duplicates(subset=['Company'], keep='last')

time_to_merge = pd.DataFrame(data_one_period[['Company','Time']])
time_to_merge.head()

data_average_with_time = pd.merge(time_to_merge,data_average, on='Company')
data_average_with_time.head()

plt.figure(figsize=(17,8))
sns.distplot(data_average[data_average['Financial Distress'] < 10]['Financial Distress'],kde=False, rug=True, color = 'blue')
plt.title('Financial Distress Histogram  of data average without outliers testing holdout')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

pd.qcut(data_average['Distress'], 3).value_counts()

data_average['status'] = np.where(data_average['Distress'] < nA bawah,0,np.where(data_average['Distress'] < na Atas, 1,2)

data_average = data_average.drop(['Distress', 'Company'], axis=1)

data_average.head()

###MODELING
##First we create a setup with the desired parameters. The report can be viewed below. The chosen methods are highlighted.

classification = setup(data_average, target = 'status', session_id = 42,normalize = True, 
                  transformation = True, 
                  ignore_low_variance = True,
                  remove_multicollinearity = True, multicollinearity_threshold = 0.75, train_size = 0.7,
                       silent = False)
compare_models()

PART 3

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

from subprocess import check_output

df = pd.read_csv("E:/Dataset Aliyya Rifanda 1401164086.csv")
df.head()

print(df.x80.unique().shape)
corrDf = df.drop(labels = ['Time','Company'], axis = 1).corr().abs()
corrDf.sort_values(by = 'Distress', inplace=True, ascending = False)
corrColumns = corrDf.drop(labels=['x80']).index.values #[corrDf['Distress'] > 0.01]
corrDf.head(n = 10)

reducedDf = df[corrColumns]
reducedDf.head()

from sklearn.preprocessing import RobustScaler, StandardScaler
scaler = StandardScaler()
trainArray = reducedDf.as_matrix() 
scaledData = trainArray
scaledData[:,1:] = scaler.fit_transform(trainArray[:,1:])

print(np.sum(scaledData[:,0] > -0.5)) # 3281 healthy
print(np.sum(scaledData[:,0] <= -0.5)) # 391 distressed cases

import seaborn as sns
sns.boxplot(data = scaledData[:,1:])

from sklearn import svm
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

uniformData = scaledData
X = uniformData[:,1:]
y = uniformData[:,0]
y_discrete = (uniformData[:,0] < -0.5).astype(int)

mdl = svm.SVR()
thresholds = np.arange(-0.5,0.5,0.1) # Try some thresholds
precisions = np.zeros_like(thresholds)
recalls = np.zeros_like(thresholds)
f1_scores = np.zeros_like(thresholds)
predicted_metric = cross_val_predict(mdl, X, y, cv = 5)
fig, ax = plt.subplots()
for i in range(len(thresholds)):
    predicted = (predicted_metric < thresholds[i]).astype(int)
    precisions[i] = precision_score(y_discrete, predicted)
    recalls[i] = recall_score(y_discrete, predicted)
    f1_scores[i] = f1_score(y_discrete, predicted)
    plt.scatter(recalls[i], precisions[i])
    ax.annotate('%0.3f' % (f1_scores[i]),(recalls[i], precisions[i]))
plt.xlabel('Recall')    
plt.ylabel('Precision')

from sklearn.model_selection import StratifiedKFold

from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt

def cvClassifier(mdl, X, y, color, name, confMat = False, confMatNormalize = True):
    skf = StratifiedKFold(n_splits = 5)
    predicted_prob = np.zeros_like(y, dtype = float)
    for train,test in skf.split(X, y):
        mdl.fit(X[train,:],y[train])
        y_prob = mdl.predict_proba(X[test,:])
        predicted_prob[test] = y_prob[:,1] #The second class 1 from 0,1 is the one to be predicted
    
    precision, recall, thresholds = precision_recall_curve(y, predicted_prob)
    plt.plot(recall, precision, color=color,label = name)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])
    plt.title('2-class Precision-Recall curve')
    plt.legend()
    
    fscore = 2*(precision*recall)/(precision + recall)
    maxFidx = np.nanargmax(fscore)
    selP = precision[maxFidx]
    selRecall = recall[maxFidx]
    selThreshold = thresholds[maxFidx]

    return predicted_prob, selP, selRecall, fscore[maxFidx], selThreshold

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y_discrete, test_size=0.3, stratify=y_discrete, random_state=42)

mdl = svm.SVC(kernel = 'linear', C=0.025, class_weight = 'balanced', probability = True)
out1 = cvClassifier(mdl, X_train, y_train, 'b','LinearSVC')

mdl = svm.SVC(C=0.5, class_weight = 'balanced', probability = True)
out2 = cvClassifier(mdl, X_train, y_train, 'g','RBFSVC')

results = [out1, out2, out3, out4]
mdlNames = ['Logit','LinearSVC','RF','RBFSVC']
fig, ax = plt.subplots()
for i in range(len(results)):
    ax.scatter(results[i][2],results[i][1])
    ax.annotate('%s %0.4f' % (mdlNames[i], results[i][3]),(results[i][2],results[i][1]))
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([0.3, 0.5])
plt.xlim([0.35, 0.65])

threshold = out2[4]
y_pred = (out2[0] > threshold).astype(int)
from sklearn.metrics import accuracy_score
acc = accuracy_score(y_train, y_pred)
print('Accuracy %0.2f' % (acc))
print('Threshold %0.3f' % (threshold))

mdl = svm.SVC(kernel = 'linear', C=0.025, class_weight = 'balanced', probability = True)
out2 = cvClassifier(mdl, X_train, y_train, 'b','LinearSVC')

y_testp = (mdl.predict_proba(X_test)[:,1] > threshold).astype(int)
acc = accuracy_score(y_test, y_testp)
print('Accuracy %0.2f' % (acc))
print('Precision %0.2f' % (precision_score(y_test,y_testp)))
print('Recall %0.2f' % (recall_score(y_test,y_testp)))





    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    







